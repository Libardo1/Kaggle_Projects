{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Test.csv file for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv(filehandler, delimiter=',', row_limit=150000,\n",
    "          output_name_template='test_%s.csv', output_path='.', keep_headers=True):\n",
    "    reader = csv.reader(filehandler, delimiter=delimiter)\n",
    "    current_piece = 1\n",
    "    current_out_path = os.path.join(\n",
    "        output_path,\n",
    "        output_name_template % current_piece\n",
    "    )\n",
    "    current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "    current_limit = row_limit\n",
    "    if keep_headers:\n",
    "        headers = next(reader)\n",
    "        current_out_writer.writerow(headers)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i + 1 > current_limit:\n",
    "            current_piece += 1\n",
    "            current_limit = row_limit * current_piece\n",
    "            current_out_path = os.path.join(\n",
    "                output_path,\n",
    "                output_name_template % current_piece\n",
    "            )\n",
    "            current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "            if keep_headers:\n",
    "                current_out_writer.writerow(headers)\n",
    "        current_out_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_csv(open('input/test.csv', 'r'),row_limit=148000,output_path='input/test_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df_train):\n",
    "    df_train['click_time']=pd.to_datetime(df_train['click_time']) #convert the click_time \n",
    "    df_train['dayofweek'] = df_train['click_time'].dt.dayofweek.astype('uint8')\n",
    "    df_train['date'] = df_train['click_time'].dt.dayofyear.astype('uint8')\n",
    "    df_train['hour']=df_train['click_time'].dt.hour.astype('uint8')\n",
    "    df_train['minute']=df_train['click_time'].dt.minute.astype('uint8')\n",
    "    df_train['second']= df_train['click_time'].dt.second.astype('uint8')\n",
    "    df_train.loc[:,'time_segment'] = 2\n",
    "    df_train.loc[df_train.loc[:,'hour']<17,'time_segment'] = 1\n",
    "    df_train.loc[df_train['hour']<14,'time_segment'] = 0\n",
    "    df_train.loc[df_train['hour']<6,'time_segment'] = 3\n",
    "    df_train.loc[df_train['hour']<2,'time_segment'] = 2\n",
    "    total_sum = len(df_train)\n",
    "    df_train['app_count'] = df_train.groupby('app')['app'].transform('count')\n",
    "    df_train.loc[:,'app_segment'] = 3\n",
    "    df_train.loc[df_train.loc[:,'app_count']<total_sum*.05,'app_segment'] = 2\n",
    "    df_train.loc[df_train.loc[:,'app_count']<total_sum*.01,'app_segment'] = 1\n",
    "    df_train.loc[df_train.loc[:,'app_count']<total_sum*.005,'app_segment'] = 0\n",
    "    df_train= df_train.drop(['app_count'], axis=1)\n",
    "    df_train['channel_count'] = df_train.groupby('channel')['channel'].transform('count')\n",
    "    df_train.loc[:,'channel_segment'] = 4\n",
    "    df_train.loc[df_train.loc[:,'channel_count']<total_sum*.08,'channel_segment'] = 3\n",
    "    df_train.loc[df_train.loc[:,'channel_count']<total_sum*.03,'channel_segment'] = 2\n",
    "    df_train.loc[df_train.loc[:,'channel_count']<total_sum*.01,'channel_segment'] = 1\n",
    "    df_train.loc[df_train.loc[:,'channel_count']<total_sum*.005,'channel_segment'] = 0\n",
    "    df_train= df_train.drop(['channel_count'], axis=1)\n",
    "    df_train['os_count'] = df_train.groupby('os')['os'].transform('count')\n",
    "    df_train.loc[:,'os_segment'] = 4\n",
    "    df_train.loc[df_train.loc[:,'os_count']<total_sum*.23,'os_segment'] = 3\n",
    "    df_train.loc[df_train.loc[:,'os_count']<total_sum*.2,'os_segment'] = 2\n",
    "    df_train.loc[df_train.loc[:,'os_count']<total_sum*.04,'os_segment'] = 1\n",
    "    df_train.loc[df_train.loc[:,'os_count']<total_sum*.02,'os_segment'] = 0\n",
    "    df_train= df_train.drop(['os_count'], axis=1)\n",
    "    df_train['device_count'] = df_train.groupby('device')['device'].transform('count')\n",
    "    df_train.loc[:,'device_segment'] = 2\n",
    "    df_train.loc[df_train.loc[:,'device_count']<total_sum*.1,'device_segment'] = 1\n",
    "    df_train.loc[df_train.loc[:,'device_count']<total_sum*.04,'device_segment'] = 0\n",
    "    df_train= df_train.drop(['device_count'], axis=1)\n",
    "\n",
    "    # Define all the groupby transformations\n",
    "    GROUPBY_AGGREGATIONS = [\n",
    "\n",
    "        # V1 - GroupBy Features #\n",
    "        #########################    \n",
    "        # Variance in hour, for ip-app-os\n",
    "        {'groupby': ['ip','app','os'], 'select': 'hour', 'agg': 'var'},\n",
    "        # Variance in date, for ip-app-os\n",
    "        {'groupby': ['ip','app','os'], 'select': 'date', 'agg': 'var'},\n",
    "        # Count, for ip-app\n",
    "        {'groupby': ['ip', 'app'], 'select': 'channel', 'agg': 'count'},        \n",
    "        # Count, for ip-app-os\n",
    "        {'groupby': ['ip', 'app', 'os'], 'select': 'channel', 'agg': 'count'},\n",
    "        # Mean hour, for ip-app-channel\n",
    "        {'groupby': ['ip','app','channel'], 'select': 'hour', 'agg': 'mean'}, \n",
    "        # Mean date, for ip-app-channel\n",
    "        {'groupby': ['ip','app','channel'], 'select': 'date', 'agg': 'mean'}, \n",
    "\n",
    "\n",
    "        # V2 - GroupBy Features #\n",
    "        #########################\n",
    "        # Average clicks on app by distinct users; is it an app they return to?\n",
    "        {'groupby': ['app'], \n",
    "         'select': 'ip', \n",
    "         'agg': lambda x: float(len(x)) / len(x.unique()), \n",
    "         'agg_name': 'AvgViewPerDistinct'\n",
    "        },\n",
    "        # How popular is the app or channel?\n",
    "        {'groupby': ['app'], 'select': 'channel', 'agg': 'count'},\n",
    "        {'groupby': ['channel'], 'select': 'app', 'agg': 'count'},\n",
    "\n",
    "        # V3 - GroupBy Features                                              #\n",
    "        # https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977 #\n",
    "        ###################################################################### \n",
    "        {'groupby': ['ip'], 'select': 'channel', 'agg': 'nunique'}, \n",
    "        {'groupby': ['ip'], 'select': 'app', 'agg': 'nunique'}, \n",
    "        {'groupby': ['ip','app'], 'select': 'os', 'agg': 'nunique'}, \n",
    "        {'groupby': ['ip'], 'select': 'device', 'agg': 'nunique'}, \n",
    "        {'groupby': ['app'], 'select': 'channel', 'agg': 'nunique'}, \n",
    "        {'groupby': ['ip', 'device', 'os'], 'select': 'app', 'agg': 'nunique'}, \n",
    "        {'groupby': ['ip','device','os'], 'select': 'app', 'agg': 'cumcount'}, \n",
    "        {'groupby': ['ip'], 'select': 'app', 'agg': 'cumcount'}, \n",
    "        {'groupby': ['ip'], 'select': 'os', 'agg': 'cumcount'}    \n",
    "    ]\n",
    "\n",
    "    # Apply all the groupby transformations\n",
    "    for spec in GROUPBY_AGGREGATIONS:\n",
    "\n",
    "        # Name of the aggregation we're applying\n",
    "        agg_name = spec['agg_name'] if 'agg_name' in spec else spec['agg']\n",
    "\n",
    "        # Name of new feature\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(spec['groupby']), agg_name, spec['select'])\n",
    "\n",
    "        # Unique list of features to select\n",
    "        all_features = list(set(spec['groupby'] + [spec['select']]))\n",
    "\n",
    "        # Perform the groupby\n",
    "        gp = df_train[all_features]. \\\n",
    "            groupby(spec['groupby'])[spec['select']]. \\\n",
    "            agg(spec['agg']). \\\n",
    "            reset_index(). \\\n",
    "            rename(index=str, columns={spec['select']: new_feature})\n",
    "\n",
    "        # Merge back to X_total\n",
    "        if 'cumcount' == spec['agg']:\n",
    "            df_train[new_feature] = gp[0].values\n",
    "        else:\n",
    "            df_train = df_train.merge(gp, on=spec['groupby'], how='left')\n",
    "\n",
    "         # Clear memory\n",
    "        del gp\n",
    "        gc.collect()\n",
    "\n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "\n",
    "        # V1\n",
    "        {'groupby': ['ip']},\n",
    "        {'groupby': ['ip', 'app']},\n",
    "        {'groupby': ['ip', 'channel']},\n",
    "        {'groupby': ['ip', 'os']},\n",
    "\n",
    "        # V3\n",
    "        {'groupby': ['ip', 'app', 'device', 'os', 'channel']},\n",
    "        {'groupby': ['ip', 'os', 'device']},\n",
    "        {'groupby': ['ip', 'os', 'device', 'app']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "\n",
    "        # Name of new feature\n",
    "        new_feature = '{}_nextClick'.format('_'.join(spec['groupby']))    \n",
    "\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        df_train[new_feature] = df_train[all_features].groupby(spec['groupby']).click_time.transform(lambda x: x.diff().shift(-1)).dt.seconds\n",
    "        gc.collect()\n",
    "\n",
    "    HISTORY_CLICKS = {\n",
    "        'identical_clicks': ['ip', 'app', 'device', 'os', 'channel'],\n",
    "        'app_clicks': ['ip', 'app']\n",
    "    }\n",
    "\n",
    "    # Go through different group-by combinations\n",
    "    for fname, fset in HISTORY_CLICKS.items():\n",
    "\n",
    "        # Clicks in the past\n",
    "        df_train['prev_'+fname] = df_train. \\\n",
    "            groupby(fset). \\\n",
    "            cumcount(). \\\n",
    "            rename('prev_'+fname)\n",
    "\n",
    "        # Clicks in the future\n",
    "        df_train['future_'+fname] = df_train.iloc[::-1]. \\\n",
    "            groupby(fset). \\\n",
    "            cumcount(). \\\n",
    "            rename('future_'+fname).iloc[::-1]\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    df_train = df_train.drop(columns=['click_time','date',\n",
    "                                      'os','channel','device','app','ip'])\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincolumns = ['ip','app', 'device', 'os', 'channel', 'click_time']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input/test_split/test_97.csv\n",
      "Processing input/test_split/test_97.csv\n",
      "Writing input/test_split/test_97fe.csv\n",
      "Reading input/test_split/test_67.csv\n",
      "Processing input/test_split/test_67.csv\n",
      "Writing input/test_split/test_67fe.csv\n",
      "Reading input/test_split/test_126.csv\n",
      "Processing input/test_split/test_126.csv\n",
      "Writing input/test_split/test_126fe.csv\n",
      "Reading input/test_split/test_13.csv\n",
      "Processing input/test_split/test_13.csv\n",
      "Writing input/test_split/test_13fe.csv\n",
      "Reading input/test_split/test_111.csv\n",
      "Processing input/test_split/test_111.csv\n",
      "Writing input/test_split/test_111fe.csv\n",
      "Reading input/test_split/test_1.csv\n",
      "Processing input/test_split/test_1.csv\n",
      "Writing input/test_split/test_1fe.csv\n",
      "Reading input/test_split/test_68.csv\n",
      "Processing input/test_split/test_68.csv\n",
      "Writing input/test_split/test_68fe.csv\n",
      "Reading input/test_split/test_78.csv\n",
      "Processing input/test_split/test_78.csv\n",
      "Writing input/test_split/test_78fe.csv\n",
      "Reading input/test_split/test_127.csv\n",
      "Processing input/test_split/test_127.csv\n",
      "Writing input/test_split/test_127fe.csv\n",
      "Reading input/test_split/test_44.csv\n",
      "Processing input/test_split/test_44.csv\n",
      "Writing input/test_split/test_44fe.csv\n",
      "Reading input/test_split/test_86.csv\n",
      "Processing input/test_split/test_86.csv\n",
      "Writing input/test_split/test_86fe.csv\n",
      "Reading input/test_split/test_118.csv\n",
      "Processing input/test_split/test_118.csv\n",
      "Writing input/test_split/test_118fe.csv\n",
      "Reading input/test_split/test_76.csv\n",
      "Processing input/test_split/test_76.csv\n",
      "Writing input/test_split/test_76fe.csv\n",
      "Reading input/test_split/test_125.csv\n",
      "Processing input/test_split/test_125.csv\n",
      "Writing input/test_split/test_125fe.csv\n",
      "Reading input/test_split/test_72.csv\n",
      "Processing input/test_split/test_72.csv\n",
      "Writing input/test_split/test_72fe.csv\n",
      "Reading input/test_split/test_99.csv\n",
      "Processing input/test_split/test_99.csv\n",
      "Writing input/test_split/test_99fe.csv\n",
      "Reading input/test_split/test_12.csv\n",
      "Processing input/test_split/test_12.csv\n",
      "Writing input/test_split/test_12fe.csv\n",
      "Reading input/test_split/test_75.csv\n",
      "Processing input/test_split/test_75.csv\n",
      "Writing input/test_split/test_75fe.csv\n",
      "Reading input/test_split/test_123.csv\n",
      "Processing input/test_split/test_123.csv\n",
      "Writing input/test_split/test_123fe.csv\n",
      "Reading input/test_split/test_100.csv\n",
      "Processing input/test_split/test_100.csv\n",
      "Writing input/test_split/test_100fe.csv\n",
      "Reading input/test_split/test_122.csv\n",
      "Processing input/test_split/test_122.csv\n",
      "Writing input/test_split/test_122fe.csv\n",
      "Reading input/test_split/test_17.csv\n",
      "Processing input/test_split/test_17.csv\n",
      "Writing input/test_split/test_17fe.csv\n",
      "Reading input/test_split/test_45.csv\n",
      "Processing input/test_split/test_45.csv\n",
      "Writing input/test_split/test_45fe.csv\n",
      "Reading input/test_split/test_52.csv\n",
      "Processing input/test_split/test_52.csv\n",
      "Writing input/test_split/test_52fe.csv\n",
      "Reading input/test_split/test_116.csv\n",
      "Processing input/test_split/test_116.csv\n",
      "Writing input/test_split/test_116fe.csv\n",
      "Reading input/test_split/test_80.csv\n",
      "Processing input/test_split/test_80.csv\n",
      "Writing input/test_split/test_80fe.csv\n",
      "Reading input/test_split/test_109.csv\n",
      "Processing input/test_split/test_109.csv\n",
      "Writing input/test_split/test_109fe.csv\n",
      "Reading input/test_split/test_90.csv\n",
      "Processing input/test_split/test_90.csv\n",
      "Writing input/test_split/test_90fe.csv\n",
      "Reading input/test_split/test_47.csv\n",
      "Processing input/test_split/test_47.csv\n",
      "Writing input/test_split/test_47fe.csv\n",
      "Reading input/test_split/test_23.csv\n",
      "Processing input/test_split/test_23.csv\n",
      "Writing input/test_split/test_23fe.csv\n",
      "Reading input/test_split/test_39.csv\n",
      "Processing input/test_split/test_39.csv\n",
      "Writing input/test_split/test_39fe.csv\n",
      "Reading input/test_split/test_124.csv\n",
      "Processing input/test_split/test_124.csv\n",
      "Writing input/test_split/test_124fe.csv\n",
      "Reading input/test_split/test_119.csv\n",
      "Processing input/test_split/test_119.csv\n",
      "Writing input/test_split/test_119fe.csv\n",
      "Reading input/test_split/test_10.csv\n",
      "Processing input/test_split/test_10.csv\n",
      "Writing input/test_split/test_10fe.csv\n",
      "Reading input/test_split/test_94.csv\n",
      "Processing input/test_split/test_94.csv\n",
      "Writing input/test_split/test_94fe.csv\n",
      "Reading input/test_split/test_92.csv\n",
      "Processing input/test_split/test_92.csv\n",
      "Writing input/test_split/test_92fe.csv\n",
      "Reading input/test_split/test_16.csv\n",
      "Processing input/test_split/test_16.csv\n",
      "Writing input/test_split/test_16fe.csv\n",
      "Reading input/test_split/test_117.csv\n",
      "Processing input/test_split/test_117.csv\n",
      "Writing input/test_split/test_117fe.csv\n",
      "Reading input/test_split/test_69.csv\n",
      "Processing input/test_split/test_69.csv\n",
      "Writing input/test_split/test_69fe.csv\n",
      "Reading input/test_split/test_115.csv\n",
      "Processing input/test_split/test_115.csv\n",
      "Writing input/test_split/test_115fe.csv\n",
      "Reading input/test_split/test_58.csv\n",
      "Processing input/test_split/test_58.csv\n",
      "Writing input/test_split/test_58fe.csv\n",
      "Reading input/test_split/test_14.csv\n",
      "Processing input/test_split/test_14.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = \"input/test_split/*.csv\"\n",
    "for fname in glob.glob(path):\n",
    "    print('Reading ' + fname)\n",
    "    df_train = pd.read_csv(fname,usecols = traincolumns, dtype=dtypes, header=0)\n",
    "    print('Processing '+ fname)\n",
    "    df_train = processing(df_train)\n",
    "    print('Writing '+ fname[:-4]+'fe.csv')\n",
    "    df_train.to_csv(fname[:-4]+'fe.csv')\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
